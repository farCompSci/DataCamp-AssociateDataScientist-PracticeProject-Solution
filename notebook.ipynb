{"cells":[{"source":"# Practical Exam: Supermarket Loyalty\n\nInternational Essentials is an international supermarket chain.\n\nShoppers at their supermarkets can sign up for a loyalty program that provides rewards each year to customers based on their spending. The more you spend the bigger the rewards. \n\nThe supermarket would like to be able to predict the likely amount customers in the program will spend, so they can estimate the cost of the rewards. \n\nThis will help them to predict the likely profit at the end of the year.\n\n## Data\n\nThe dataset contains records of customers for their last full year of the loyalty program.\n\n| Column Name | Criteria                                                |\n|-------------|---------------------------------------------------------|\n|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|\n","metadata":{},"id":"0a8ca74a-b235-4034-9c1c-3159336a39d5","cell_type":"markdown"},{"source":"# Task 1\n\nBefore you fit any models, you will need to make sure the data is clean. \n\nThe table below shows what the data should look like. \n\nCreate a cleaned version of the dataframe. \n\n - You should start with the data in the file \"loyalty.csv\". \n\n - Your output should be a dataframe named `clean_data`. \n\n - All column names and values should match the table below.\n\n| Column Name | Criteria                                                |\n|-------------|---------------------------------------------------------|\n|customer_id | Unique identifier for the customer. </br>Missing values are not possible due to the database structure. |\n|spend | Continuous. </br>The total spend of the customer in their last full year. This can be any positive value to two decimal places. </br>Missing values should be replaced with 0. |\n|first_month | Continuous. </br>The amount spent by the customer in their first month of the year. This can be any positive value, rounded to two decimal places. </br>Missing values should be replaced with 0. |\n| items_in_first_month | Discrete. </br>The number of items purchased in the first month. Any integer value greater than or equal to zero. </br>Missing values should be replaced by 0. |  \n| region | Nominal. </br>The geographic region that the customer is based in. One of four values Americas, Asia/Pacific, Europe, Middle East/Africa. </br>Missing values should be replaced with \"Unknown\". |\n| loyalty_years | Oridinal. </br>The number of years the customer has been a part of the loyalty program. One of five ordered categories, '0-1', '1-3', '3-5', '5-10', '10+'. </br>Missing values should be replaced with '0-1'.|\n| joining_month | Nominal. </br>The month the customer joined the loyalty program. One of 12 values \"Jan\", \"Feb\", \"Mar\", \"Apr\", etc. </br>Missing values should be replaced with \"Unknown\".|\n| promotion | Nominal. </br>Did the customer join the loyalty program as part of a promotion? Either 'Yes' or 'No'. </br>Missing values should be replaced with 'No'.|","metadata":{},"id":"790f8bb8-76fb-44fd-8078-c62909a91b2b","cell_type":"markdown"},{"source":"# Use this cell to write your code for Task 1\nimport pandas as pd \nimport numpy as np\n\nfilepath = \"loyalty.csv\"\nloyalty_df = pd.read_csv(filepath)\n\n# Make sure there is no negative spend\nassert len(loyalty_df[loyalty_df['spend'] < 0]['spend']) == 0, 'Negative Spend Detected!'\nassert loyalty_df['spend'].isna().sum() == 0, 'Null Values found for Spend! Needs Addressing!'\n# If for some reason, we missed something\nloyalty_df['spend'].fillna(0,inplace=True)\n\n# Cleaning First Month \nloyalty_df.loc[loyalty_df['first_month'] == \".\",'first_month'] = 0\nassert len(loyalty_df[loyalty_df['first_month'] == \".\"]) == 0, 'Some NaN Values were missed'\nloyalty_df['first_month'] = loyalty_df['first_month'].astype('float64')\n\n# Making sure that the min value is greater than 0\nassert loyalty_df['first_month'].min() >= 0, 'Some Values are Negative'\nassert loyalty_df['first_month'].isna().sum() == 0, 'Null Values found for First Month! Needs Addressing'\n\n# Cleaning Items in First Month \nassert loyalty_df['items_in_first_month'].dtype == 'int64', 'Items in First Month is in the Wrong Data Type'\nassert len(loyalty_df[loyalty_df['items_in_first_month']<=0]) == 0, 'Error! Negative `Items in First Month`'\nassert loyalty_df['items_in_first_month'].isna().sum() == 0, \"Null Values found in `Items in First Month`\"\nloyalty_df['items_in_first_month'].fillna(0, inplace=True)\n\n# Cleaning Regions Data \nloyalty_df['region'] = loyalty_df['region'].astype('category')\nregions = ['Americas', 'Asia/Pacific', 'Europe', 'Middle East/Africa']\nassert len(loyalty_df[loyalty_df['region'].isin(regions)]) == len(loyalty_df['region']),'Values Found Outside Specified Regions'\nassert loyalty_df['region'].isna().sum() == 0, 'Null Values found in Region'\n# In case testing code introduces missing values\nloyalty_df['region'].fillna('Unknown',inplace=True)\n\n# Cleaning Loyalty Years\nloyalty_df['loyalty_years'] = loyalty_df['loyalty_years'].astype('O')\nloyalty_df['loyalty_years'].unique()\n# Creating ordered categories\ncategories = ['0-1', '1-3', '3-5', '5-10', '10+']\nloyalty_df['loyalty_years'] = pd.Categorical(loyalty_df['loyalty_years'],categories=categories,ordered=True)\nassert len(loyalty_df[~loyalty_df['loyalty_years'].isin(categories)]) == 0, 'Values Found Outside Loyalty Years Category'\nassert loyalty_df['loyalty_years'].isna().sum() == 0, 'Null Values found in Loyalty Years'\nloyalty_df['loyalty_years'].fillna('0-1',inplace=True)\n\n# Cleaning Joining Month \nloyalty_df['joining_month'].fillna('Unknown',inplace=True)\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Unknown']\nassert len(loyalty_df[loyalty_df['joining_month'].isin(months)]) == len(loyalty_df['joining_month']), 'Values Found Outside Joining Months'\nassert loyalty_df['joining_month'].isna().sum() == 0, 'Values Found Outside Joining Months'\nloyalty_df['joining_month'] = loyalty_df['joining_month'].astype('category')\n\n# Cleaning Promotion \nloyalty_df['promotion'] = loyalty_df['promotion'].str.title()\nloyalty_df['promotion'] = loyalty_df['promotion'].astype('category')\nassert loyalty_df['promotion'].isna().sum() == 0, 'Null Values found in Promotion'\n\nclean_data = loyalty_df.copy()","metadata":{"executionCancelledAt":null,"executionTime":4299,"lastExecutedAt":1735933062433,"lastExecutedByKernel":"8e06e7e7-62d4-49a7-868e-9be59f666286","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use this cell to write your code for Task 1\nimport pandas as pd \nimport numpy as np\n\nfilepath = \"loyalty.csv\"\nloyalty_df = pd.read_csv(filepath)\n\n# Make sure there is no negative spend\nassert len(loyalty_df[loyalty_df['spend'] < 0]['spend']) == 0, 'Negative Spend Detected!'\nassert loyalty_df['spend'].isna().sum() == 0, 'Null Values found for Spend! Needs Addressing!'\n# If for some reason, we missed something\nloyalty_df['spend'].fillna(0,inplace=True)\n\n# Cleaning First Month \nloyalty_df.loc[loyalty_df['first_month'] == \".\",'first_month'] = 0\nassert len(loyalty_df[loyalty_df['first_month'] == \".\"]) == 0, 'Some NaN Values were missed'\nloyalty_df['first_month'] = loyalty_df['first_month'].astype('float64')\n\n# Making sure that the min value is greater than 0\nassert loyalty_df['first_month'].min() >= 0, 'Some Values are Negative'\nassert loyalty_df['first_month'].isna().sum() == 0, 'Null Values found for First Month! Needs Addressing'\n\n# Cleaning Items in First Month \nassert loyalty_df['items_in_first_month'].dtype == 'int64', 'Items in First Month is in the Wrong Data Type'\nassert len(loyalty_df[loyalty_df['items_in_first_month']<=0]) == 0, 'Error! Negative `Items in First Month`'\nassert loyalty_df['items_in_first_month'].isna().sum() == 0, \"Null Values found in `Items in First Month`\"\nloyalty_df['items_in_first_month'].fillna(0, inplace=True)\n\n# Cleaning Regions Data \nloyalty_df['region'] = loyalty_df['region'].astype('category')\nregions = ['Americas', 'Asia/Pacific', 'Europe', 'Middle East/Africa']\nassert len(loyalty_df[loyalty_df['region'].isin(regions)]) == len(loyalty_df['region']),'Values Found Outside Specified Regions'\nassert loyalty_df['region'].isna().sum() == 0, 'Null Values found in Region'\n# In case testing code introduces missing values\nloyalty_df['region'].fillna('Unknown',inplace=True)\n\n# Cleaning Loyalty Years\nloyalty_df['loyalty_years'] = loyalty_df['loyalty_years'].astype('O')\nloyalty_df['loyalty_years'].unique()\n# Creating ordered categories\ncategories = ['0-1', '1-3', '3-5', '5-10', '10+']\nloyalty_df['loyalty_years'] = pd.Categorical(loyalty_df['loyalty_years'],categories=categories,ordered=True)\nassert len(loyalty_df[~loyalty_df['loyalty_years'].isin(categories)]) == 0, 'Values Found Outside Loyalty Years Category'\nassert loyalty_df['loyalty_years'].isna().sum() == 0, 'Null Values found in Loyalty Years'\nloyalty_df['loyalty_years'].fillna('0-1',inplace=True)\n\n# Cleaning Joining Month \nloyalty_df['joining_month'].fillna('Unknown',inplace=True)\nmonths = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Unknown']\nassert len(loyalty_df[loyalty_df['joining_month'].isin(months)]) == len(loyalty_df['joining_month']), 'Values Found Outside Joining Months'\nassert loyalty_df['joining_month'].isna().sum() == 0, 'Values Found Outside Joining Months'\nloyalty_df['joining_month'] = loyalty_df['joining_month'].astype('category')\n\n# Cleaning Promotion \nloyalty_df['promotion'] = loyalty_df['promotion'].str.title()\nloyalty_df['promotion'] = loyalty_df['promotion'].astype('category')\nassert loyalty_df['promotion'].isna().sum() == 0, 'Null Values found in Promotion'\n\nclean_data = loyalty_df.copy()","outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"1a87447c-cbb5-4e8b-94b7-75cf54690a59","nodeType":"const"},"quickFilterText":""}}}},"id":"523dd03c-8591-4cc6-b750-d71da287745f","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# Task 2 \n\nThe team at International Essentials have told you that they have always believed that the number of years in the loyalty scheme is the biggest driver of spend. \n\nProducing a table showing the difference in the average spend by number of years in the loyalty programme along with the variance to investigate this question for the team.\n\n - You should start with the data in the file 'loyalty.csv'.\n\n - Your output should be a data frame named `spend_by_years`. \n\n - It should include the three columns `loyalty_years`, `avg_spend`, `var_spend`. \n\n - Your answers should be rounded to 2 decimal places.   ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"08b695b7-67db-48fb-8b14-e12bb5a9620e","cell_type":"markdown"},{"source":"# Use this cell to write your code for Task 2\nloyalty_df = pd.read_csv('loyalty.csv')\nspend_by_years = loyalty_df.groupby('loyalty_years')['spend'].agg(['mean','var']).sort_values(by='loyalty_years').round(2).reset_index()\nspend_by_years.rename(columns={'mean':'avg_spend','var':'var_spend'},inplace=True)\nprint(spend_by_years)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1735933062486,"lastExecutedByKernel":"8e06e7e7-62d4-49a7-868e-9be59f666286","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use this cell to write your code for Task 2\nloyalty_df = pd.read_csv('loyalty.csv')\nspend_by_years = loyalty_df.groupby('loyalty_years')['spend'].agg(['mean','var']).sort_values(by='loyalty_years').round(2).reset_index()\nspend_by_years.rename(columns={'mean':'avg_spend','var':'var_spend'},inplace=True)\nprint(spend_by_years)","outputsMetadata":{"0":{"height":143,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"cc590298-c483-4253-bef1-a352933cbd5e","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"  loyalty_years  avg_spend  var_spend\n0           0-1     110.56       9.30\n1           1-3     129.31       9.65\n2           10+     117.41      16.72\n3           3-5     124.55      11.09\n4          5-10     135.15      14.10\n"}]},{"source":"# Task 3\n\nFit a baseline model to predict the spend over the year for each customer.\n\n 1. Fit your model using the data contained in “train.csv” </br></br>\n\n 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `base_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values.","metadata":{},"id":"7113acde-8a74-487a-8983-f0c93003d945","cell_type":"markdown"},{"source":"from sklearn.linear_model import LinearRegression\n\n# Task 3 - Base Model\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\n\n# Defining the categorical columns\ncat_cols = ['promotion', 'joining_month', 'region', 'loyalty_years']\n\n# Process training data\ntrain_data[cat_cols] = train_data[cat_cols].astype('category')\ntrain_data = pd.get_dummies(train_data, drop_first=True)\n\n# Process test data\ntest_data[cat_cols] = test_data[cat_cols].astype('category')\ntest_data = pd.get_dummies(test_data, drop_first=True)\n\n# Train base model\nX_train = train_data.drop('spend', axis=1).values\ny_train = train_data['spend'].values\nbase_model = LinearRegression()\nbase_model.fit(X_train, y_train)\n\n# Make predictions\nX_test = test_data.values\nbase_model_predictions = base_model.predict(X_test)\n\n# Create final base result DataFrame\nbase_result = pd.DataFrame({\n    'customer_id': test_data['customer_id'],\n    'spend': base_model_predictions.round(2)  \n})","metadata":{"executionCancelledAt":null,"executionTime":553,"lastExecutedAt":1735933063039,"lastExecutedByKernel":"8e06e7e7-62d4-49a7-868e-9be59f666286","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import LinearRegression\n\n# Task 3 - Base Model\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\n\n# Defining the categorical columns\ncat_cols = ['promotion', 'joining_month', 'region', 'loyalty_years']\n\n# Process training data\ntrain_data[cat_cols] = train_data[cat_cols].astype('category')\ntrain_data = pd.get_dummies(train_data, drop_first=True)\n\n# Process test data\ntest_data[cat_cols] = test_data[cat_cols].astype('category')\ntest_data = pd.get_dummies(test_data, drop_first=True)\n\n# Train base model\nX_train = train_data.drop('spend', axis=1).values\ny_train = train_data['spend'].values\nbase_model = LinearRegression()\nbase_model.fit(X_train, y_train)\n\n# Make predictions\nX_test = test_data.values\nbase_model_predictions = base_model.predict(X_test)\n\n# Create final base result DataFrame\nbase_result = pd.DataFrame({\n    'customer_id': test_data['customer_id'],\n    'spend': base_model_predictions.round(2)  \n})","outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"1a87447c-cbb5-4e8b-94b7-75cf54690a59","nodeType":"const"},"quickFilterText":""}},"1":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"1a87447c-cbb5-4e8b-94b7-75cf54690a59","nodeType":"const"},"quickFilterText":""}}}},"id":"79a77f11-b09b-4d70-893b-535dfde919b2","cell_type":"code","execution_count":3,"outputs":[]},{"source":"# Task 4\n\nFit a comparison model to predict the spend over the year for each customer.\n\n 1. Fit your model using the data contained in “train.csv” </br></br>\n\n 2. Use “test.csv” to predict new values based on your model. You must return a dataframe named `compare_result`, that includes `customer_id` and `spend`. The `spend` column must be your predicted values.","metadata":{},"id":"44033abf-a603-479e-8663-9a96fefee5a2","cell_type":"markdown"},{"source":"# Use this cell to write your code for Task 4\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Using GBR for predictions\ncompare_model = GradientBoostingRegressor(\n    n_estimators=100,\n    max_depth=3,\n    learning_rate=0.1,\n    random_state=42\n)\ncompare_model.fit(X_train, y_train)\n\n# Storing the predictions\ncompare_model_results = compare_model.predict(X_test)\n\nassert compare_model_results.shape[0] == test_data.shape[0], 'Shape Mismatch between Prediction and Test Files'\n\n# Make predictions\ncompare_model_results = compare_model.predict(X_test)\n\n# Create final compare result DataFrame\ncompare_result = pd.DataFrame({\n    'customer_id': test_data['customer_id'],\n    'spend': compare_model_results.round(2)\n})\n\ncompare_result","metadata":{"executionCancelledAt":null,"executionTime":141,"lastExecutedAt":1735933130110,"lastExecutedByKernel":"8e06e7e7-62d4-49a7-868e-9be59f666286","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Use this cell to write your code for Task 4\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Using GBR for predictions\ncompare_model = GradientBoostingRegressor(\n    n_estimators=100,\n    max_depth=3,\n    learning_rate=0.1,\n    random_state=42\n)\ncompare_model.fit(X_train, y_train)\n\n# Storing the predictions\ncompare_model_results = compare_model.predict(X_test)\n\nassert compare_model_results.shape[0] == test_data.shape[0], 'Shape Mismatch between Prediction and Test Files'\n\n# Make predictions\ncompare_model_results = compare_model.predict(X_test)\n\n# Create final compare result DataFrame\ncompare_result = pd.DataFrame({\n    'customer_id': test_data['customer_id'],\n    'spend': compare_model_results.round(2)\n})\n\ncompare_result","outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"quickFilterText":"","customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"0339cfb8-dd7b-40a9-9927-9b57b8b9b5b7","nodeType":"const"}}}}},"id":"731cfa01-2709-413a-ac19-611e73e37c75","cell_type":"code","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"customer_id","type":"integer"},{"name":"spend","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249],"customer_id":[5,7,16,17,19,34,37,46,47,50,54,56,66,67,70,75,77,83,99,103,105,107,119,123,124,125,129,136,142,156,157,158,163,167,168,169,175,184,188,190,200,201,213,219,224,233,234,239,242,249,255,256,258,263,266,271,272,279,280,297,298,311,312,314,315,320,321,327,329,337,341,343,344,345,355,375,382,385,386,388,390,393,394,398,401,409,412,414,415,421,436,440,442,446,448,458,465,473,478,485,491,493,496,499,511,512,513,515,516,525,535,551,558,573,575,578,581,583,587,588,596,606,607,612,622,629,632,633,635,646,649,651,658,659,661,662,682,685,687,695,700,702,705,710,715,726,738,747,748,753,755,766,768,769,773,775,776,789,793,798,799,802,808,814,818,828,833,834,838,848,852,853,856,858,861,866,868,871,872,885,886,888,892,893,898,906,907,919,923,932,936,940,951,970,977,979,987,994,997,999,1001,1008,1011,1019,1026,1028,1029,1031,1039,1040,1042,1049,1052,1058,1069,1074,1081,1104,1111,1112,1120,1122,1123,1126,1127,1128,1136,1139,1145,1146,1151,1159,1164,1170,1172,1179,1182,1191,1196,1197,1198,1199,1202,1206,1207,1216,1225,1231,1242,1243],"spend":[140.84,148.86,139.79,151.05,153.64,66.16,145.03,140.75,146.17,137.52,143.87,132.24,132.01,142.98,134.66,123.89,71.32,142.44,77.47,141.98,140.74,141.94,77.64,71.11,150.26,151.36,77.67,132.01,74.98,151.36,137.54,137.52,66.09,148.55,146.17,148.72,69.04,69.94,137.52,150.95,148.72,136.94,141.98,151.36,151.36,123.89,77.49,138.47,144.98,148.77,123.58,142.01,137.52,140.87,142.43,143.87,146.16,134.68,145.03,151.36,123.85,149.32,142.44,127.98,153.58,137.52,77.67,142.93,140.42,128.09,143.43,77.8,146.47,134.68,140.87,137.54,146.16,80.01,151.36,151.36,137.52,71.28,134.92,145.55,74.95,66.09,131.07,139.82,136.94,136.94,123.85,74.92,137.54,137.43,140.76,148.8,137.52,141.98,151.32,151.36,142.44,146.16,137.5,143.06,77.49,132.03,74.98,150.77,141.32,140.42,150.95,141.55,141.32,80.01,66.15,140.93,138.47,137.51,139.72,145.55,151.36,151.36,149.11,148.72,72.99,146.16,151.36,131.97,143.06,146.47,80.01,72.99,153.58,143.6,66.08,150.95,139.16,150.95,150.77,151.36,153.58,129.78,150.74,138.39,80.01,131.57,137.5,137.5,123.53,74.95,129.7,151.36,137.5,123.53,77.67,153.43,143.25,148.8,150.99,146.47,123.85,141.32,151.36,137.5,146.16,143.23,129.7,77.67,137.5,147.49,132.2,138.47,148.8,137.51,143.23,69.03,72.99,142.1,74.96,148.8,143.7,132.03,151.36,141.98,77.84,146.16,146.16,66.08,151.36,151.08,130.98,142.44,148.72,79.98,137.5,145.55,77.8,143.87,125.78,74.98,148.77,77.67,138.47,142.27,141.32,149.77,143.06,126.74,140.87,134.15,141.81,136.94,143.42,79.97,74.98,150.95,141.98,139.7,148.08,137.5,146.17,140.46,77.67,143.87,145.03,72.99,134.92,146.17,134.38,66.08,66.15,134.68,138.5,71.26,148.8,66.36,138.5,129.71,74.95,140.87,151.36,123.85,137.51,138.47,132.22,134.92,148.77,136.87,129.87,148.77]}},"total_rows":250,"truncation_type":null},"text/plain":"     customer_id   spend\n0              5  140.84\n1              7  148.86\n2             16  139.79\n3             17  151.05\n4             19  153.64\n..           ...     ...\n245         1216  134.92\n246         1225  148.77\n247         1231  136.87\n248         1242  129.87\n249         1243  148.77\n\n[250 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>spend</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>140.84</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>148.86</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>139.79</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17</td>\n      <td>151.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>153.64</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>1216</td>\n      <td>134.92</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>1225</td>\n      <td>148.77</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>1231</td>\n      <td>136.87</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>1242</td>\n      <td>129.87</td>\n    </tr>\n    <tr>\n      <th>249</th>\n      <td>1243</td>\n      <td>148.77</td>\n    </tr>\n  </tbody>\n</table>\n<p>250 rows × 2 columns</p>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":11}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}